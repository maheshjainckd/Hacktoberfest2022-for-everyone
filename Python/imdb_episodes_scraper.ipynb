{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0WV6qKcsv1e"
      },
      "source": [
        "## A Simple Web Scraper created using Python and BS4 which will scrape imdb website to get episode details for a series which will later be used for data analysis and model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x3yk8pGxV2u"
      },
      "source": [
        "For Connecting gdrive to colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxN-B86Utbex",
        "outputId": "041839e5-77eb-4b8b-9923-c8c0ae158aa0"
      },
      "outputs": [],
      "source": [
        "#code to connect to google drive\n",
        "# from google.colab import drive \n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG9nD-YsxVSt",
        "outputId": "adbbca33-4b34-4443-8bc0-f070d734203c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/gdrive/MyDrive/web_scraping_data/'\n",
            "d:\\Programming\\Python\\Web Scraping\\imdb_episodes\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/web_scraping_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCxmWi1QxxOW"
      },
      "source": [
        "Code starts From Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "udtHIGSlJr6q"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import sys \n",
        "import imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C7G084PZKjAQ"
      },
      "outputs": [],
      "source": [
        "# Testing BS4\n",
        "# Retreive website Data\n",
        "# url = 'https://www.imdb.com/title/tt0458290/episodes?season=1'\n",
        "# response = requests.get(url)\n",
        "# season_page = BeautifulSoup(response.content)\n",
        "# # season_page\n",
        "# episode_tiles = season_page.findAll('div',attrs={'class':'info'})\n",
        "# # episode_tiles[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eqfKefIcpqSO"
      },
      "outputs": [],
      "source": [
        "def getSeasonsCount(input_url):\n",
        "    \"\"\"\n",
        "    Function to get total no of seasons present in the input series\n",
        "    Parameters: \n",
        "    input_url: URL to scrape from\n",
        "    Returns: \n",
        "    Total number of seasons (int)\n",
        "    \"\"\"\n",
        "    url = input_url\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "    except: \n",
        "        print(\"Error in connecting to the website\")\n",
        "        return -1\n",
        "    season_page = BeautifulSoup(response.content)\n",
        "    seasonsSelect = season_page.find('div',attrs={'class':'episode-list-select'}).find('select', attrs={'id':'bySeason'})\n",
        "    optionsVal = [int(option.text.strip()) for option in seasonsSelect.findAll('option')]\n",
        "    return max(optionsVal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fm5iUvOpQz9W"
      },
      "outputs": [],
      "source": [
        "# For scraping from all the seasons \n",
        "def scrape_all_data(input_url):\n",
        "    \"\"\"\n",
        "    Function that will scrape all the episodes data from a series\n",
        "    Parameters:\n",
        "    input_url: URL to scrape from\n",
        "    Returns:\n",
        "    dictionary containing scraped data\n",
        "    \"\"\"\n",
        "    no_of_seasons = getSeasonsCount(input_url)\n",
        "    if no_of_seasons < 0 :\n",
        "        return -1\n",
        "    season_no_list = []\n",
        "    episode_no_list = []\n",
        "    episode_name_list = []\n",
        "    episode_airdate_list = []\n",
        "    episode_score_list = []\n",
        "    episode_votes_list = []\n",
        "    for season in range(no_of_seasons):\n",
        "        season_no = season+1\n",
        "        print(f\"Parsing Season {season_no}\")\n",
        "        # Retreive website Data\n",
        "        url = f'{input_url}?season={season_no}'\n",
        "        response = requests.get(url)\n",
        "        season_page = BeautifulSoup(response.content)\n",
        "        episodes = season_page.findAll('div',attrs={'class':'info'})\n",
        "        print(f'There are {len(episodes)} episodes in season {season_no}')\n",
        "        for episode_no, episode in enumerate(episodes):\n",
        "            # episode_name \n",
        "            episode_name = episode.strong.a.text.strip()\n",
        "            # episode_airdata \n",
        "            episode_airdate = episode.find('div',attrs={'class':'airdate'}).text.strip().replace('.','')\n",
        "            # episode_score \n",
        "            try:\n",
        "                episode_score = episode.find(\n",
        "                'div',attrs={'class':'ipl-rating-widget'}\n",
        "                ).find(\n",
        "                    'div',attrs={'class':'ipl-rating-star small'}\n",
        "                ).find('span',attrs={'class':'ipl-rating-star__rating'}).text\n",
        "            except:\n",
        "                episode_score= 0\n",
        "        \n",
        "            # episode_votes \n",
        "            try:\n",
        "                episode_votes = episode.find(\n",
        "                'div',attrs={'class':'ipl-rating-widget'}\n",
        "                ).find(\n",
        "                    'div',attrs={'class':'ipl-rating-star small'}\n",
        "                ).find('span',attrs={'class':'ipl-rating-star__total-votes'}).text.strip('()').replace(',','')\n",
        "            except:\n",
        "                episode_votes = 0\n",
        "            # print(f'------------------- Episode {episode_no+1} -------------------')\n",
        "            # print(f'Episode Name: {episode_name}')\n",
        "            # print(f'Episode Date: {episode_airdate}')\n",
        "            # print(f'Episode Score: {episode_score}')\n",
        "            # print(f'Episode Votes: {episode_votes}')\n",
        "            # print('--------------------------------------------------')\n",
        "            \n",
        "            # creating lists\n",
        "            season_no_list.append(season_no)\n",
        "            episode_no_list.append(episode_no+1)\n",
        "            episode_name_list.append(episode_name)\n",
        "            episode_airdate_list.append(episode_airdate)\n",
        "            episode_score_list.append(episode_score)\n",
        "            episode_votes_list.append(episode_votes)\n",
        "\n",
        "    print('Parsing Done')\n",
        "    # creating dictionary from list \n",
        "    data_dict = {\n",
        "        'season_no': season_no_list,\n",
        "        'episode no': episode_no_list,\n",
        "        'episode_name': episode_name_list,\n",
        "        'episode_airdate': episode_airdate_list,\n",
        "        'episode_score': episode_score_list,\n",
        "        'episode_votes': episode_votes_list\n",
        "    }\n",
        "\n",
        "    return data_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2yk_40Scua0h"
      },
      "outputs": [],
      "source": [
        "# Used imdb package to get movie url for input name\n",
        "import imdb\n",
        "def getURlByName(series_name):\n",
        "    \"\"\"\n",
        "    Function to get url of the series entered by user \n",
        "    Parameters: \n",
        "    series_name: Name of the series to scrape\n",
        "    Returns: \n",
        "    URL: URL of the website to scrape\n",
        "    \"\"\"\n",
        "    ia = imdb.IMDb()\n",
        "    try:\n",
        "        results = ia.search_movie(series_name)\n",
        "        URL = ia.get_imdbURL(results[0])\n",
        "        return URL\n",
        "    except:\n",
        "        print(\"No Such Series Found\")\n",
        "        sys.exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jHUfYv91pRXW"
      },
      "outputs": [],
      "source": [
        "# Function to save the dataset in a csv file\n",
        "def saveDataset(df,name):\n",
        "    name = name.replace(' ','_').lower()\n",
        "    df.to_csv(f'{name}_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "Ezeu8yjKPK1w",
        "outputId": "573f3202-ee0e-4f3d-9509-9f8c9df23e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL for Stranger Things: https://www.imdb.com/title/tt4574334/episodes\n",
            "----------------- SCRAPING STARTED ----------------------------\n",
            "Parsing Season 1\n",
            "There are 8 episodes in season 1\n",
            "Parsing Season 2\n",
            "There are 9 episodes in season 2\n",
            "Parsing Season 3\n",
            "There are 8 episodes in season 3\n",
            "Parsing Season 4\n",
            "There are 9 episodes in season 4\n",
            "Parsing Season 5\n",
            "There are 1 episodes in season 5\n",
            "Parsing Done\n",
            "----------------- SCRAPING DONE ------------------------------\n",
            "----------------- SAVING DATA --------------------------------\n",
            "----------------- DATASET SAVED ------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>season_no</th>\n",
              "      <th>episode no</th>\n",
              "      <th>episode_name</th>\n",
              "      <th>episode_airdate</th>\n",
              "      <th>episode_score</th>\n",
              "      <th>episode_votes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Chapter One: The Vanishing of Will Byers</td>\n",
              "      <td>15 Jul 2016</td>\n",
              "      <td>8.5</td>\n",
              "      <td>23751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Chapter Two: The Weirdo on Maple Street</td>\n",
              "      <td>15 Jul 2016</td>\n",
              "      <td>8.4</td>\n",
              "      <td>21125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Chapter Three: Holly, Jolly</td>\n",
              "      <td>15 Jul 2016</td>\n",
              "      <td>8.8</td>\n",
              "      <td>20983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Chapter Four: The Body</td>\n",
              "      <td>15 Jul 2016</td>\n",
              "      <td>8.9</td>\n",
              "      <td>20586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Chapter Five: The Flea and the Acrobat</td>\n",
              "      <td>15 Jul 2016</td>\n",
              "      <td>8.7</td>\n",
              "      <td>19503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   season_no  episode no                              episode_name  \\\n",
              "0          1           1  Chapter One: The Vanishing of Will Byers   \n",
              "1          1           2   Chapter Two: The Weirdo on Maple Street   \n",
              "2          1           3               Chapter Three: Holly, Jolly   \n",
              "3          1           4                    Chapter Four: The Body   \n",
              "4          1           5    Chapter Five: The Flea and the Acrobat   \n",
              "\n",
              "  episode_airdate episode_score episode_votes  \n",
              "0     15 Jul 2016           8.5         23751  \n",
              "1     15 Jul 2016           8.4         21125  \n",
              "2     15 Jul 2016           8.8         20983  \n",
              "3     15 Jul 2016           8.9         20586  \n",
              "4     15 Jul 2016           8.7         19503  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataframe with pandas \n",
        "# URL will be of this format\n",
        "# URL = 'https://www.imdb.com/title/tt0458290/episodes'\n",
        "series_name = input(\"Input name of series to generate data: \")\n",
        "URL = getURlByName(series_name)+'episodes'\n",
        "print(f'URL for {series_name}: {URL}')\n",
        "print('----------------- SCRAPING STARTED ----------------------------')\n",
        "data = scrape_all_data(URL)\n",
        "if data == -1:\n",
        "    print('Error in scraping')\n",
        "    sys.exit(0)\n",
        "print('----------------- SCRAPING DONE ------------------------------')\n",
        "df = pd.DataFrame(data)\n",
        "print('----------------- SAVING DATA --------------------------------')\n",
        "saveDataset(df,series_name)\n",
        "print('----------------- DATASET SAVED ------------------------------')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTcB2vIQzhy-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "imdb_episodes_scraper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('.aivenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a6aa66c812e617fe7a976665f04a60359ca2a7b0fa520cb928d0804dda4cd71e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
